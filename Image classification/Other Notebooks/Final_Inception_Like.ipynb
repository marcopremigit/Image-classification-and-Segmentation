{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpF09AVL_iV5"
   },
   "source": [
    "# Functions and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOavXZWC_e_U"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRlSN1gt_e_U"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tensorflow.keras.utils\n",
    "\n",
    "SEED = 5\n",
    "np.seed = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwbMG-Iz_e_U"
   },
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzJVdbze_e_U"
   },
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKN0ax7f_e_V"
   },
   "outputs": [],
   "source": [
    "def shuffle_validation_weighted(base_dir, split = 0.2, reset = False):\n",
    "    # Create validation_set weighted on number of occurrences in training_set\n",
    "    train_dir = os.path.join(base_dir, 'training')\n",
    "    valid_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "    if not reset:\n",
    "        # First identify training and validation dir\n",
    "        if not os.path.exists(valid_dir):\n",
    "            os.makedirs(valid_dir)\n",
    "\n",
    "        # Count elements in each dir in training\n",
    "        class_and_card = {name: len(os.listdir(os.path.join(train_dir, name))) for name in os.listdir(train_dir) if\n",
    "                          os.path.join(train_dir, name)}\n",
    "\n",
    "        print(class_and_card)\n",
    "        # Get images per class wrt total images\n",
    "        class_and_card_validation = {name: int(class_and_card[name] * split) for name in class_and_card}\n",
    "\n",
    "        print(class_and_card_validation)\n",
    "        # Select images to move\n",
    "        for key, item in class_and_card_validation.items():\n",
    "            source_dir = os.path.join(train_dir, key)\n",
    "            images = os.listdir(source_dir)\n",
    "            np.random.shuffle(images)\n",
    "            target_dir = os.path.join(valid_dir, key)\n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            for i in range(item):\n",
    "                shutil.move(os.path.join(source_dir, images[i]), target_dir)\n",
    "    else:\n",
    "        # Restore initial state\n",
    "        # For each class, move images to train_dir in respective folders\n",
    "        classes = [name for name in os.listdir(valid_dir)]\n",
    "        for class_name in classes:\n",
    "            source_dir = os.path.join(valid_dir, class_name)\n",
    "            images = os.listdir(source_dir)\n",
    "            target_dir = os.path.join(train_dir, class_name)\n",
    "            for img in images:\n",
    "                shutil.move(os.path.join(source_dir, img), target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7Lb1rNz_nIP"
   },
   "outputs": [],
   "source": [
    "cwd = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFczks_2_rm3"
   },
   "source": [
    "# Model and Dataset Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2Izh_ch_e_V"
   },
   "outputs": [],
   "source": [
    "# We executed a scoped GridSearch on the following parameters\n",
    "# Dense Layers -> [1, 2, 3] (tested parameters)\n",
    "# Neurons Number -> [256, 512, 1024]\n",
    "# Starting Filter Number -> [8, 16, 32, 64]\n",
    "# Depth -> range(1, 8)\n",
    "# Image Size -> [256] (same for heigth and width)\n",
    "# Batch Size -> [8, 16, 32]\n",
    "# Valid Percentage -> [0.2]\n",
    "# Starting Learning Rate -> [1e-3, 1e-4]\n",
    "# LR1 values -> // not tested\n",
    "# LR2 values -> // not tested\n",
    "# Below we have the best results\n",
    "batch_size = 16\n",
    "img_w, img_h = 256, 256\n",
    "num_classes = 3\n",
    "valid_split_perc = 0.2\n",
    "depth = 4\n",
    "start_f = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcG2ZSmw_e_V",
    "outputId": "e0dcdcc3-6480-4495-a82b-47e45877c410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 1900, '1': 1897, '2': 1817}\n",
      "{'0': 285, '1': 284, '2': 272}\n"
     ]
    }
   ],
   "source": [
    "shuffle_validation_weighted(cwd, split=valid_split_perc, reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Drca3nPs_e_V",
    "outputId": "f4cac199-3324-4f2b-c29d-a5372e73e5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4773 images belonging to 3 classes.\n",
      "Found 841 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode=\"nearest\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(os.path.join(cwd, 'training'),\n",
    "                                                    target_size=(img_w, img_h),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    seed = SEED)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(os.path.join(cwd, 'validation'),\n",
    "                                                target_size=(img_w, img_h),\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                class_mode='categorical',\n",
    "                                                seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iUyv96V_e_W",
    "outputId": "704686c1-874b-4ff1-807c-c3ebe4eaa8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 254, 254, 16) 448         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 254, 254, 16) 64          conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 127, 127, 16) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 127, 127, 4)  68          max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 127, 127, 4)  68          max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 127, 127, 16) 272         max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 127, 127, 16) 592         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 127, 127, 16) 1616        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 127, 127, 48) 0           conv2d_43[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 127, 127, 48) 192         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 63, 63, 48)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 63, 63, 8)    392         max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 63, 63, 8)    392         max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 63, 63, 32)   1568        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 63, 63, 32)   2336        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 63, 63, 32)   6432        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 63, 63, 96)   0           conv2d_48[0][0]                  \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 63, 63, 96)   384         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 31, 31, 96)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 31, 31, 16)   1552        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 31, 31, 16)   1552        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 31, 31, 64)   6208        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 31, 31, 64)   9280        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 31, 31, 64)   25664       conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 31, 31, 192)  0           conv2d_53[0][0]                  \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 31, 31, 192)  768         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 15, 15, 192)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 15, 15, 32)   6176        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 15, 15, 32)   6176        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 15, 15, 128)  24704       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 15, 15, 128)  36992       conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 15, 15, 128)  102528      conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 15, 15, 384)  0           conv2d_58[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 15, 15, 384)  1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 7, 7, 384)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18816)        0           max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          9634304     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            1539        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,873,803\n",
      "Trainable params: 9,872,331\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.Input(shape=(img_h, img_w, 3))\n",
    "\n",
    "pre_conv = tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                 kernel_size=(3,3),\n",
    "                                 strides=(1, 1),\n",
    "                                 padding='valid',\n",
    "                                 activation='relu')(input_layer)\n",
    "pre_norm = tf.keras.layers.BatchNormalization()(pre_conv)\n",
    "pre_pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(pre_norm)\n",
    "\n",
    "temp_first = pre_pool\n",
    "\n",
    "# Layers are concatenated.\n",
    "# 1 Parallel layer > 1x1 | 3x3\n",
    "# 2 Parallel layer > 1x1 | 5x5\n",
    "# 1 Parallel layer > 1x1 \n",
    "# A batch normalisation is added for overfitting\n",
    "for i in range(depth):\n",
    "    layer1 = tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    strides=(1, 1),\n",
    "                                    padding='same',\n",
    "                                    activation='relu')(temp_first)\n",
    "    layer2_0 = tf.keras.layers.Conv2D(filters=start_f//4, # Less filters to reduce spatial dimension\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      strides=(1, 1),\n",
    "                                      padding='same',\n",
    "                                      activation='relu')(temp_first)\n",
    "    layer2_1 = tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      strides=(1, 1),\n",
    "                                      padding='same',\n",
    "                                      activation='relu')(layer2_0)\n",
    "    layer3_0 = tf.keras.layers.Conv2D(filters=start_f//4,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      strides=(1, 1),\n",
    "                                      padding='same',\n",
    "                                      activation='relu')(temp_first)\n",
    "    layer3_1 = tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                      kernel_size=(5, 5),\n",
    "                                      strides=(1, 1),\n",
    "                                      padding='same',\n",
    "                                      activation='relu')(layer3_0)\n",
    "\n",
    "    layer_out = tf.keras.layers.concatenate([layer1, layer2_1, layer3_1], axis = 3)\n",
    "    batch_norm = tf.keras.layers.BatchNormalization()(layer_out)\n",
    "    pooling = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(batch_norm)\n",
    "    temp_first = pooling\n",
    "    start_f *= 2 # This increases the number of filters\n",
    "\n",
    "flat_layer = tf.keras.layers.Flatten()(pooling)\n",
    "dense_0 = tf.keras.layers.Dense(512, activation='relu')(flat_layer)\n",
    "drop_layer = tf.keras.layers.Dropout(0.3)(dense_0)\n",
    "dense_1 = tf.keras.layers.Dense(3, activation='softmax')(drop_layer)\n",
    "\n",
    "model = Model(input_layer, dense_1)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensorflow.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aULgnXBg_e_W"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKRtUazc_e_W"
   },
   "outputs": [],
   "source": [
    "cb_early_stopper = EarlyStopping(monitor = 'val_accuracy', patience = 8, mode = 'max')\n",
    "cb_checkpointer = ModelCheckpoint(filepath = 'best.hdf5', monitor = 'val_accuracy', save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvHP6GFUBDOi"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TP72RB02_e_W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = 4773\n",
    "nb_val_samples = 841\n",
    "\n",
    "epochs_fine = 200\n",
    "\n",
    "hist = model.fit(train_generator,\n",
    "                 steps_per_epoch=nb_train_samples // batch_size,\n",
    "                 epochs=epochs_fine,\n",
    "                 validation_data=val_generator,\n",
    "                 validation_steps=nb_val_samples // batch_size,\n",
    "                 callbacks=[cb_checkpointer, cb_early_stopper],\n",
    "                 verbose = 2)\n",
    "\n",
    "plot_hist(hist)\n",
    "\n",
    "model.load_weights(\"best.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO3qdLx1BP-J"
   },
   "source": [
    "# Result Evaluation Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NF7wVbxE_e_X"
   },
   "outputs": [],
   "source": [
    "model.load_weights('inception_80.hdf5')\n",
    "\n",
    "test_dir = os.path.join(cwd, 'test')\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_data_gen.flow_from_directory(test_dir, target_size=(img_h, img_w), \n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='categorical',\n",
    "                                                 classes = None,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=False)\n",
    "test_gen.reset()\n",
    "\n",
    "predictions = model.predict_generator(test_gen, len(test_gen), verbose=1)\n",
    "\n",
    "results = {}\n",
    "\n",
    "images = test_gen.filenames\n",
    "i = 0\n",
    "\n",
    "for p in predictions:\n",
    "  prediction = np.argmax(p)\n",
    "  import ntpath\n",
    "  image_name = ntpath.basename(images[i])\n",
    "  results[image_name] = str(prediction)\n",
    "  i = i + 1\n",
    "  \n",
    "create_csv(results, cwd)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Top_Basic_Inception_HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
