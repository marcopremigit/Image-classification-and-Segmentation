{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC8xl81rdg7I"
   },
   "source": [
    "# Combine the 2 networks into one prediction over the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOBDSA9BnFIx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from numpy.random import shuffle\n",
    "import shutil\n",
    "import patchify as p\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "from matplotlib import cm\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YT1i2asB8iz"
   },
   "source": [
    "# Script Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook only shows how to compute and merge the predictions from the two networks.\n",
    "The two networks were trained on different notebooks similar to the ones provided in the submission, except for a custom generator function which is reported below.\n",
    "\n",
    "This generator provides an additional parameter called 'class_index' which removes the pixels of the non-pertinent class from masks (e.g., for the 'crop' network it would replace every value=2 - pixel with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_generator(tf.keras.utils.Sequence):\n",
    "\n",
    "  # Beware class_index destroys the mask for given index\n",
    "\n",
    "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
    "               preprocessing_function=None, class_index = None):\n",
    "    subset_file = os.path.join(dataset_dir, 'img.txt')\n",
    "    \n",
    "    with open(subset_file, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "    \n",
    "    subset_filenames = []\n",
    "    for line in lines:\n",
    "      subset_filenames.append(line.strip()) \n",
    "\n",
    "    self.which_subset = which_subset\n",
    "    self.dataset_dir = dataset_dir\n",
    "    self.subset_filenames = subset_filenames\n",
    "    self.img_generator = img_generator\n",
    "    self.mask_generator = mask_generator\n",
    "    self.preprocessing_function = preprocessing_function\n",
    "    self.class_index = class_index\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.subset_filenames)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.__data__generation(index)\n",
    "        \n",
    "  def __data__generation(self, index):\n",
    "    \n",
    "    if index >= len(self.subset_filenames):\n",
    "      return None, None\n",
    "    curr_filename = self.subset_filenames[index]\n",
    "    \n",
    "    img = Image.open(os.path.join(self.dataset_dir, 'Images',\n",
    "                                      curr_filename))\n",
    "    mask = Image.fromarray(read_rgb_mask(os.path.join(self.dataset_dir, 'Masks',\n",
    "                                      curr_filename)))\n",
    "\n",
    "    img_arr = np.array(img)\n",
    "    mask_arr = np.array(mask)\n",
    "    \n",
    "\n",
    "    if len(mask_arr.shape) == 2: \n",
    "      mask_arr = np.expand_dims(mask_arr, -1)\n",
    "\n",
    "    out_mask = None\n",
    "\n",
    "    if self.which_subset == 'training':\n",
    "      if self.img_generator is not None and self.mask_generator is not None:\n",
    "        \n",
    "        img_t = self.img_generator.get_random_transform(img_arr.shape)\n",
    "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
    "        \n",
    "        out_mask = np.zeros_like(mask_arr)\n",
    "        for c in np.unique(mask_arr):\n",
    "          if c > 0:\n",
    "            curr_class_arr = np.float32(mask_arr == c)\n",
    "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, img_t)\n",
    "            \n",
    "            curr_class_arr = np.uint8(curr_class_arr)\n",
    "            \n",
    "            curr_class_arr = curr_class_arr * c \n",
    "            out_mask += curr_class_arr\n",
    "    else:\n",
    "      out_mask = mask_arr\n",
    "    \n",
    "    if self.class_index is not None:\n",
    "        out_mask[out_mask == self.class_index] = 0\n",
    "    \n",
    "    if self.preprocessing_function is not None:\n",
    "        img_arr = self.preprocessing_function(img_arr)\n",
    "      \n",
    "    return img_arr, np.float32(out_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKYjoayM4Vhy"
   },
   "source": [
    "Converting in classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QI3ABYWp4HR"
   },
   "outputs": [],
   "source": [
    "# Converts images to RGB to Target\n",
    "def read_rgb_mask(img_path):\n",
    "    '''\n",
    "    img_path: path to the mask file\n",
    "    Returns the numpy array containing target values\n",
    "    '''\n",
    "\n",
    "    mask_img = Image.open(img_path)\n",
    "    mask_arr = np.array(mask_img)\n",
    "\n",
    "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
    "\n",
    "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
    "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
    "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
    "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
    "\n",
    "    return new_mask_arr\n",
    "\n",
    "def mask_to_rgb(mask_arr):\n",
    "\n",
    "    mask = np.squeeze(mask_arr, axis=2)\n",
    "    new_mask_arr = np.zeros((mask_arr.shape[0], mask_arr.shape[1], 3), dtype=mask_arr.dtype)\n",
    "\n",
    "    new_mask_arr[np.where(mask == 0)] = np.array([0, 0, 0])\n",
    "    new_mask_arr[np.where(mask == 1)] = np.array([255, 255, 255])\n",
    "    new_mask_arr[np.where(mask == 2)] = np.array([216, 67, 82])\n",
    "\n",
    "    return new_mask_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTlvojWQHlfn"
   },
   "source": [
    "RLE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6vBas5xHk8Z"
   },
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - foreground, 0 - background\n",
    "    Returns run length as string formatted\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kfiCuOxCGLM"
   },
   "source": [
    "Script for dividing training set and validation set. Create .txts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWsIWtwc4K9p"
   },
   "outputs": [],
   "source": [
    "# plant = 'haricot' OR 'mais' OR 'all'\n",
    "# mode = 'all' OR 'bipbip' OR 'pead' OR 'roseau' OR 'weedelec'\n",
    "def split_set(base_dir = './Development_Dataset/Training', split = 0.2, plant = 'haricot', mode = 'all', is_test = False):\n",
    "    \n",
    "    plants = []\n",
    "    \n",
    "    if plant == 'all':\n",
    "        plants = ['haricot', 'mais']\n",
    "    else:\n",
    "        plants.append(plant)\n",
    "    \n",
    "    if not is_test:\n",
    "      datasets = []\n",
    "      files_ls = []\n",
    "    \n",
    "      if mode == 'all':\n",
    "          datasets.extend(['Bipbip', 'Pead', 'Roseau', 'Weedelec'])\n",
    "      else:\n",
    "          datasets.append(mode.capitalize())\n",
    "        \n",
    "      for dataset in datasets:\n",
    "        for p in plants:\n",
    "            images_dir = os.path.join(base_dir, dataset, p.capitalize(), 'Images')\n",
    "            files_ls_temp = os.listdir(images_dir)\n",
    "            files_ls.extend([filename.split('.')[0] + '\\n' for filename in files_ls_temp])\n",
    "            \n",
    "      shuffle(files_ls)\n",
    "          \n",
    "      final_element = int(len(files_ls) * split)\n",
    "      val_ls = files_ls[:final_element]\n",
    "      train_ls = files_ls[final_element:]\n",
    "      print('# images for training: ' + str(len(train_ls)))\n",
    "      print('# images for validation: ' + str(len(val_ls)))\n",
    "          \n",
    "      with open(os.path.join(base_dir,\"train.txt\"), \"w\") as train_txt:\n",
    "          train_txt.writelines(train_ls)\n",
    "          \n",
    "      with open(os.path.join(base_dir,\"val.txt\"), \"w\") as val_txt:\n",
    "          val_txt.writelines(val_ls)\n",
    "    \n",
    "    else:\n",
    "\n",
    "      datasets = []\n",
    "      datasets.extend(['Bipbip', 'Pead', 'Roseau', 'Weedelec'])\n",
    "\n",
    "      path = os.path.join(base_dir, \"temptest\")\n",
    "      if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "      os.mkdir(path)\n",
    "    \n",
    "      plants = ['haricot', 'mais']\n",
    "\n",
    "      for pl in plants:\n",
    "        for dataset in datasets:\n",
    "          images_dir = os.path.join(base_dir, dataset, pl.capitalize(), 'Images')\n",
    "          files_ls_temp = os.listdir(images_dir)\n",
    "          for file in files_ls_temp:\n",
    "            shutil.copy(os.path.join(images_dir, file), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czvLV_c8i0xo"
   },
   "source": [
    "Code for handling tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sJnaDM0i0f1",
    "outputId": "12052e76-1e4c-4f24-bed9-092b5558c1d4"
   },
   "outputs": [],
   "source": [
    "def produce_patches(img_o, mask_o, window_size, step = 20, with_pad = True):\n",
    "  # From img get array of patches\n",
    "  # From mask get array of patches\n",
    "  # window_size = size of a patch\n",
    "  # step = stride\n",
    "  # with_pad = Adds pad for precise recostruction\n",
    "\n",
    "  img_patches = None\n",
    "  mask_patches = None\n",
    "\n",
    "  img = None\n",
    "  mask = None\n",
    "  pads = None\n",
    "  if with_pad:\n",
    "    if img_o is not None:\n",
    "      # Prepare image in case not proportional step\n",
    "      diff = np.array([img_o.shape[0] - window_size, img_o.shape[1] - window_size])\n",
    "      # Since we need exact patches... padding!\n",
    "      times = np.ceil(diff/step)\n",
    "      dims = np.multiply([step, step], times)\n",
    "      pads = window_size+dims-np.array(img_o.shape[:2])\n",
    "      img = np.pad(img_o, ((0, int(pads[0])), (0, int(pads[1])), (0, 0)), 'constant')\n",
    "    if mask_o is not None:\n",
    "      # Prepare image in case not proportional step\n",
    "      diff = np.array([mask_o.shape[0] - window_size, mask_o.shape[1] - window_size])\n",
    "      # Since we need exact patches... padding!\n",
    "      times = np.ceil(diff/step)\n",
    "      dims = np.multiply([step, step], times)\n",
    "      pads = window_size+dims-np.array(mask_o.shape[:2])\n",
    "      mask = np.pad(mask_o, ((0, int(pads[0])), (0, int(pads[1])), (0, 0)), 'constant')\n",
    "  else:\n",
    "    img = img_o\n",
    "    mask = mask_o\n",
    "\n",
    "  if img is not None:\n",
    "    img_patches = []\n",
    "    img_patches_result = p.patchify(img, (window_size, window_size, 3), step=step)\n",
    "    for w in range(img_patches_result.shape[0]):\n",
    "      for h in range(img_patches_result.shape[1]):\n",
    "        img_patches.append(img_patches_result[w, h, 0])\n",
    "    img_patches = np.array(img_patches)\n",
    "  if mask is not None:\n",
    "    mask_patches = []\n",
    "    mask_patches_result = p.patchify(mask, (window_size, window_size, 1), step=step)\n",
    "    for w in range(mask_patches_result.shape[0]):\n",
    "      for h in range(mask_patches_result.shape[1]):\n",
    "        mask_patches.append(mask_patches_result[w, h, 0])\n",
    "    mask_patches = np.array(mask_patches)\n",
    "\n",
    "  return img_patches, mask_patches, pads\n",
    "\n",
    "def restore_with_interp(img_pa, or_shape, win_size, step, wi, hi):\n",
    "    #Recostruct from patches\n",
    "    # or_shape = original shape with len 3\n",
    "    # win_size = window_size\n",
    "    # step = step\n",
    "    # wi = number of patches row wise\n",
    "    # he = number of patches height wise\n",
    "    \n",
    "    i = 0\n",
    "    margin = win_size - step\n",
    "    if len(or_shape) == 2:\n",
    "      raise Exception(\"or_shape must be 3D\")\n",
    "    result = np.zeros(or_shape, np.float32)\n",
    "\n",
    "    dtype = img_pa.dtype\n",
    "\n",
    "    img_p = None\n",
    "    if dtype != np.float32:\n",
    "      img_p = img_pa.astype(np.float32)\n",
    "    else:\n",
    "      img_p = img_pa\n",
    "\n",
    "    for w in range(wi):\n",
    "      for h in range(hi):\n",
    "        result[w*win_size-w*margin:(w+1)*win_size-w*margin, h*win_size-h*margin:(h+1)*win_size-h*margin] += img_p[i]\n",
    "        i+=1\n",
    "\n",
    "    for w in range(wi):\n",
    "      for h in range(hi):\n",
    "        if w == 0:\n",
    "          result[0:margin, step*(h+1):step*(h+1)+margin] /= 2\n",
    "        result[w*step+margin:(w+1)*step, step*(h+1):step*(h+1)+margin] /= 2\n",
    "\n",
    "    for h in range(wi):\n",
    "      for w in range(hi):\n",
    "        if h == 0:\n",
    "          result[step*(w+1):step*(w+1)+margin, 0:margin] /= 2\n",
    "        result[step*(w+1):step*(w+1)+margin, h*step+margin:(h+1)*step] /= 2\n",
    "\n",
    "    for w in range(1,wi):\n",
    "      for h in range(1,hi):\n",
    "        result[win_size*w-margin*w:win_size*w-margin*(w-1), win_size*h-margin*h:win_size*h-margin*(h-1)] /= 4\n",
    "\n",
    "    if dtype != np.float32:\n",
    "      result = result.astype(dtype)\n",
    "      \n",
    "    return result\n",
    "  \n",
    "def reconstruct_patches(img_patches, mask_patches, or_shape, pads, step=20):\n",
    "  # From img patches get img\n",
    "  # From mask patches get mask\n",
    "  # or_shape = [width, height] like of original image\n",
    "  # step = stride7\n",
    "\n",
    "  restoring = or_shape\n",
    "  channel = or_shape[2]\n",
    "  or_shape = np.uint(np.array(or_shape[:2]) + pads)\n",
    "  or_shape = [or_shape[0], or_shape[1], channel]\n",
    "\n",
    "  # Values for restoration\n",
    "  win_size = img_patches.shape[1]\n",
    "  \n",
    "  img = None\n",
    "  mask = None\n",
    "  if img_patches is not None:\n",
    "    wh = img_patches.shape[0]\n",
    "    wi = int((or_shape[0] - img_patches.shape[1])//step + 1)\n",
    "    he = int((or_shape[1] - img_patches.shape[2])//step + 1)\n",
    "    if wh != he * wi:\n",
    "      raise Exception(\"hw != h*w\")\n",
    "    \n",
    "    img = restore_with_interp(img_patches, or_shape, win_size, step, wi, he)\n",
    "    img = img[:restoring[0],:restoring[1]]\n",
    "\n",
    "  if mask_patches is not None:\n",
    "    wh = mask_patches.shape[0]\n",
    "    wi = int((or_shape[0] - mask_patches.shape[1])//step + 1)\n",
    "    he = int((or_shape[1] - mask_patches.shape[2])//step + 1)\n",
    "    if wh != hi * we:\n",
    "      raise Exception(\"hw != h*w\")\n",
    "      \n",
    "    mask = restore_with_interp(mask_patches, or_shape, win_size, step, wi, he)\n",
    "    mask = mask[:restoring[0],:restoring[1]]\n",
    "  return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyPQb48XCt-k"
   },
   "source": [
    "Code used for counting number of pixel in mask. Optimized to be fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eDcOCgKqJfS"
   },
   "outputs": [],
   "source": [
    "# Return a dictionary of distinct values of mask\n",
    "def f_pp(mask, nmax=1000):\n",
    "    iai32 = mask.ravel()\n",
    "    colors = list(set(iai32))\n",
    "    counts = []\n",
    "    match = None\n",
    "    for i in range(0, len(colors)):\n",
    "        counts.append(0)\n",
    "        match = iai32 == colors[i]\n",
    "        counts[i] = np.count_nonzero(match)\n",
    "    colors = [np.uint8(col) for col in colors]\n",
    "    return {a[0] : a[1] for a in zip(colors, counts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQtgaurxC23P"
   },
   "source": [
    "Utility script to save the patch in a folder and having them swiftly prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg_FD9Z677S8"
   },
   "outputs": [],
   "source": [
    "def save_patch(save_fld, sub, dat_dir, win_size, exclude = 0.95):\n",
    "  # This method create patches for entire dataset and save it in a folder\n",
    "  # A text file with all the images is also created.\n",
    "\n",
    "  subset_file = None\n",
    "  if sub == 'training':\n",
    "      subset_file = os.path.join(dat_dir, 'train.txt')\n",
    "  elif sub == 'validation':\n",
    "      subset_file = os.path.join(dat_dir, 'val.txt')\n",
    "    \n",
    "  with open(subset_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "  subset_filenames = []\n",
    "  for line in lines:\n",
    "    subset_filenames.append(line.strip())\n",
    "  \n",
    "  image_names = []\n",
    "  folder = \"Images\"\n",
    "  folder_m = \"Masks\"\n",
    "\n",
    "  if os.path.exists(save_fld):\n",
    "    shutil.rmtree(save_fld)\n",
    "  os.makedirs(save_fld)\n",
    "  os.makedirs(os.path.join(save_fld, folder))\n",
    "  os.makedirs(os.path.join(save_fld, folder_m))\n",
    "  \n",
    "  for i in range(0, len(subset_filenames)):\n",
    "    curr_filename = subset_filenames[i]\n",
    "    split_filename = curr_filename.split('_')\n",
    "    curr_dataset = split_filename[0]\n",
    "    curr_plant = split_filename[1]\n",
    "    \n",
    "    if curr_dataset == 'Roseau':\n",
    "        img = Image.open(os.path.join(dat_dir, curr_dataset, curr_plant.capitalize(), 'Images',\n",
    "                                      curr_filename + '.png'))\n",
    "    else:\n",
    "        img = Image.open(os.path.join(dat_dir, curr_dataset, curr_plant.capitalize(), 'Images',\n",
    "                                      curr_filename + '.jpg'))\n",
    "    \n",
    "    mask = Image.fromarray(read_rgb_mask(os.path.join(dat_dir, curr_dataset, curr_plant.capitalize(),\n",
    "                                                      'Masks', curr_filename + '.png')))\n",
    "\n",
    "    img_arr = np.array(img)\n",
    "    mask_arr = np.array(mask)\n",
    "    mask_arr = np.expand_dims(mask_arr, -1)\n",
    "    \n",
    "    # We put WITH_PAD = FALSE in order to avoid creating overlapping images in training.\n",
    "    # This is done because introducing overlapping images would cause redundancy.\n",
    "    img_p, mask_p, _ = produce_patches(img_arr, mask_arr, win_size, step=win_size, with_pad=False)\n",
    "    img_arr = []\n",
    "    mask_arr = []\n",
    "    sum = win_size*win_size\n",
    "    for j in range(0, mask_p.shape[0]):\n",
    "      dict_colors = f_pp(mask_p[j])\n",
    "      if dict_colors.get(0, 0) < sum*exclude:\n",
    "         filename = \"img_\" + str(len(image_names)) + \".png\"\n",
    "         img = Image.fromarray(img_p[j])\n",
    "         mask = Image.fromarray(mask_to_rgb(mask_p[j]))\n",
    "         img.save(os.path.join(save_fld, folder, filename))\n",
    "         mask.save(os.path.join(save_fld, folder_m, filename))\n",
    "         image_names.append(filename + '\\n')\n",
    "    print(i+1, \"images processed. Still: \", len(subset_filenames)-i-1)\n",
    "  with open(os.path.join(save_fld,\"img.txt\"), \"w\") as val_txt:\n",
    "    val_txt.writelines(image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1bU_fkh3x4c"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMg2bAT13v2L"
   },
   "outputs": [],
   "source": [
    "cwd = './'\n",
    "\n",
    "dataset_dir = \"Development_Dataset\"\n",
    "training_dir = os.path.join(dataset_dir, \"Training\")\n",
    "test_dir = os.path.join(dataset_dir, \"Test_Dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcbblkg5Ck7R",
    "outputId": "5cfce585-4d7e-4342-8c96-081e77de6019"
   },
   "outputs": [],
   "source": [
    "mode = \"bipbip\"\n",
    "plant = \"mais\"\n",
    "\n",
    "split_set(base_dir= training_dir, mode=mode, plant=plant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKykxOHBEZvG"
   },
   "source": [
    "Dictionary with array colors, used for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vg_-NFlGDXqE"
   },
   "outputs": [],
   "source": [
    "dict_rgb = {\n",
    "    \"background\" : [254, 124, 18],\n",
    "    \"crop\" : [255, 255, 255],\n",
    "    \"weed\" : [216, 67, 82]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpg-PkOydVgV"
   },
   "outputs": [],
   "source": [
    "win_size_1 = 512\n",
    "win_size_2 = 800\n",
    "step1 = win_size_1 - 6\n",
    "step2 = win_size_2 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhQZAEarYMh9"
   },
   "outputs": [],
   "source": [
    "def create_model_1(depth, start_f, num_classes, input_shape, trainable = None):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Encoder\n",
    "    vgg = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model.add(vgg)\n",
    "    \n",
    "    start_f = start_f\n",
    "        \n",
    "    # Decoder\n",
    "    for i in range(depth):\n",
    "        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                         kernel_size=(3, 3),\n",
    "                                         strides=(1, 1),\n",
    "                                         padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "        start_f = start_f // 2\n",
    "\n",
    "    # Prediction Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n",
    "                                     kernel_size=(1, 1),\n",
    "                                     strides=(1, 1),\n",
    "                                     padding='same',\n",
    "                                     activation='softmax'))\n",
    "    \n",
    "    if trainable is not None:\n",
    "      for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "      model.load_weights(trainable)\n",
    "    else:\n",
    "      for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(depth, start_f, num_classes, input_shape, trainable = None):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Encoder\n",
    "    vgg = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model.add(vgg)\n",
    "    \n",
    "    start_f = start_f\n",
    "        \n",
    "    # Decoder\n",
    "    for i in range(depth):\n",
    "        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=start_f,\n",
    "                                         kernel_size=(3, 3),\n",
    "                                         strides=(1, 1),\n",
    "                                         padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "        start_f = start_f // 2\n",
    "\n",
    "    # Prediction Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n",
    "                                     kernel_size=(1, 1),\n",
    "                                     strides=(1, 1),\n",
    "                                     padding='same',\n",
    "                                     activation='softmax'))\n",
    "    \n",
    "    if trainable is not None:\n",
    "      for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "      model.load_weights(trainable)\n",
    "    else:\n",
    "      for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k97OK6CRnFJS",
    "outputId": "41460730-06c3-4bcb-ddb0-1a15baeeaa32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_network = create_model_1(depth=5, \n",
    "                     start_f=512, \n",
    "                     num_classes=3, input_shape = (win_size_1, win_size_1, 3), trainable = \"/content/class1/weights\")\n",
    "\n",
    "first_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nzluHdxuuam",
    "outputId": "f8013460-4786-4cd0-893c-e37be4617a4c"
   },
   "outputs": [],
   "source": [
    "second_network = create_model_2(depth=5, \n",
    "                     start_f=512, \n",
    "                     num_classes=3, input_shape = (win_size_2, win_size_2, 3), trainable = \"/content/class2/weights\")\n",
    "\n",
    "first_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogxrsGSLwjG6"
   },
   "outputs": [],
   "source": [
    "split_set(base_dir=test_dir, mode='bipbip', plant='haricot', is_test = True)\n",
    "temptest = os.path.join(test_dir, \"temptest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MeVmSL2CuwYG",
    "outputId": "fcb057a8-233b-46e0-c44a-526f59054fe7"
   },
   "outputs": [],
   "source": [
    "images = os.listdir(temptest)\n",
    "submission_dict = {}\n",
    "\n",
    "if len(images) > 0:\n",
    "  idx = 0\n",
    "  for img_name in images:\n",
    "\n",
    "    imgs = Image.open(os.path.join(temptest, img_name))\n",
    "    img_arr = np.array(imgs)\n",
    "    or_shape = img_arr.shape\n",
    "\n",
    "    predictions_class_1 = []\n",
    "    predictions_class_2 = []\n",
    "    # Replace win_size with input size of network, step with win_size - overlap(typically 6)\n",
    "    img_p1, _, pads1 = produce_patches(img_arr, None, win_size_1, step = step1)\n",
    "    img_p2, _, pads2 = produce_patches(img_arr, None, win_size_2, step = step2)\n",
    "  \n",
    "    for i in range(img_p1.shape[0]):\n",
    "      out_sigmoid1 = first_network.predict(x=tf.expand_dims(img_p1[i], axis=0))\n",
    "      predictions_class_1.append(out_sigmoid1)\n",
    "        \n",
    "    \n",
    "    for i in range(img_p2.shape[0]):\n",
    "      out_sigmoid2 = second_network.predict(x=tf.expand_dims(img_p2[i], axis=0))\n",
    "      predictions_class_2.append(out_sigmoid2)\n",
    "\n",
    "    predictions_class_1 = np.concatenate(predictions_class_1, axis = 0)\n",
    "    predictions_class_2 = np.concatenate(predictions_class_2, axis = 0)\n",
    "    # Only replace step\n",
    "    predicted_class_1, _ = reconstruct_patches(predictions_class_1, None, or_shape, pads1, step=step1)\n",
    "    predicted_class_2, _ = reconstruct_patches(predictions_class_2, None, or_shape, pads2, step=step2)\n",
    "\n",
    "    # Class Index 1 destroyed in 1\n",
    "    # Class Index 2 destroyed in 2\n",
    "    # Bring index 1 in 2\n",
    "    print(img_name)\n",
    "    background_prediction = (predicted_class_1[:,:,0] + predicted_class_2[:,:,0])/2\n",
    "    predicted_class = np.stack([background_prediction, predicted_class_2[:,:,1], predicted_class_1[:,:,2]], axis=-1)\n",
    "    predicted_class = tf.argmax(predicted_class, -1)\n",
    "    mask_arr = predicted_class\n",
    "    mask_arr = np.array(mask_arr)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4)\n",
    "    ax[0].imshow(np.uint8(np.argmax(predicted_class_1, -1)))\n",
    "    ax[1].imshow(np.uint8(np.argmax(predicted_class_2, -1)))\n",
    "    ax[2].imshow(np.uint8(mask_arr))\n",
    "    ax[3].imshow(np.uint8(img_arr))\n",
    "    plt.show()\n",
    "\n",
    "    split_filename = img_name.split('_')\n",
    "    curr_dataset = split_filename[0]\n",
    "    curr_plant = split_filename[1]\n",
    "    curr_plant = curr_plant.capitalize()\n",
    "\n",
    "    img_name = os.path.splitext(img_name)\n",
    "    img_name = img_name[0]\n",
    "\n",
    "    submission_dict[img_name] = {}\n",
    "    submission_dict[img_name]['shape'] = or_shape\n",
    "    submission_dict[img_name]['team'] = curr_dataset\n",
    "    submission_dict[img_name]['crop'] = curr_plant\n",
    "    submission_dict[img_name]['segmentation'] = {}\n",
    "\n",
    "    # RLE encoding\n",
    "    # crop\n",
    "    rle_encoded_crop = rle_encode(mask_arr == 1)\n",
    "    # weed\n",
    "    rle_encoded_weed = rle_encode(mask_arr == 2)\n",
    "\n",
    "    submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n",
    "    submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n",
    "\n",
    "    # Please notice that in this example we have a single prediction.\n",
    "    # For the competition you have to provide segmentation for each of\n",
    "    # the test images.\n",
    "\n",
    "        # Finally, save the results into the submission.json file\n",
    "import json\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(submission_dict, f)\n",
    "\n",
    "import zipfile\n",
    "zipfile.ZipFile('submission.zip', mode='w').write(\"submission.json\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Predict.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
